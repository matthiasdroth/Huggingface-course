{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fd32d9",
   "metadata": {},
   "source": [
    "# Sharing models and tokenizers\n",
    "## [The Hugging Face Hub](https://huggingface.co/course/chapter4/1?fw=pt)\n",
    "The [Hugging Face Hub](https://huggingface.co/) ‚Äì our main website ‚Äì is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5.\n",
    "\n",
    "The models in the Hub are not limited to ü§ó Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few.\n",
    "\n",
    "Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage.\n",
    "\n",
    "Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets.\n",
    "\n",
    "The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately.\n",
    "\n",
    "The video below shows how to navigate the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a17b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d16116",
   "metadata": {},
   "source": [
    "Having a huggingface.co account is required to follow along this part, as we‚Äôll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)!\n",
    "\n",
    "## [Using pretrained models](https://huggingface.co/course/chapter4/2?fw=pt)\n",
    "The Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code. Let's take a look at how to actually use one of these models, and how to contribute back to the community.\n",
    "\n",
    "Let's say we're looking for a French-based model that can perform mask filling.\n",
    "\n",
    "<img style=\"float=center;\" src=\"images/ebebabd431fdcc0bafbd74bfee8a90884627381d4e666da1a89381bab108fee1.gif\">\n",
    "\n",
    "We select the `camembert-base` checkpoint to try it out. The identifier `camembert-base` is all we need to start using it! As you've seen in previous chapters, we can instantiate it using the `pipeline()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2e7bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206e035188124e2b8e0b77c33d88b1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae8f35315bf49f99c3c293ba29cd448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21284ffe995041989310fe2646040739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6307fa4d6c5d41f7a89781dd53e96c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aadcf7",
   "metadata": {},
   "source": [
    "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it‚Äôs going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n",
    "\n",
    "<img style=\"float=center;\" src=\"images/b213f6e22a9303a2abefe6de81cc40baf95b497eb2ed818da5bee620147952a8.png\">\n",
    "\n",
    "You can also instantiate the checkpoint using the model architecture directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6d43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741957f8",
   "metadata": {},
   "source": [
    "However, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the `CamemBERT` architecture, using the `Auto*` classes makes switching checkpoints simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bf07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65453d",
   "metadata": {},
   "source": [
    "> <font color=\"darkgreen\">When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card.</font>\n",
    "\n",
    "## [Sharing pretrained models](https://huggingface.co/course/chapter4/3?fw=pt)\n",
    "In the steps below, we'll take a look at the easiest ways to share pretrained models to the ü§ó Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7d302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/9yY3RB_GSPM\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/9yY3RB_GSPM\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd345f",
   "metadata": {},
   "source": [
    "We encourage all users that train models to contribute by sharing them with the community ‚Äî sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done!\n",
    "\n",
    "There are three ways to go about creating new model repositories:\n",
    "- Using the `push_to_hub` API\n",
    "- Using the `huggingface_hub` Python library\n",
    "- Using the web interface\n",
    "\n",
    "Once you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections.\n",
    "\n",
    "### Using the `push_to_hub` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc10f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Zh0FfmVrKX0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Zh0FfmVrKX0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092cc76",
   "metadata": {},
   "source": [
    "The simplest way to upload files to the Hub is by leveraging the `push_to_hub` API.\n",
    "\n",
    "Before going further, you'll need to generate an authentication token so that the `huggingface_hub` API knows who you are and what namespaces you have write access to. Make sure you are in an environment where you have `transformers` installed (see [Setup](https://huggingface.co/course/chapter0)). If you are in a notebook, you can use the following function to login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24041d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/matthias/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc51c1",
   "metadata": {},
   "source": [
    "In a terminal, you can run:\n",
    "```terminal\n",
    "huggingface-cli login\n",
    "```\n",
    "In both cases, you should be prompted for your username and password, which are the same ones you use to log in to the Hub. If you do not have a Hub profile yet, you should create one [here](https://huggingface.co/join).\n",
    "\n",
    "Great! You now have your authentication token stored in your cache folder. Let's create some repositories!\n",
    "\n",
    "If you have played around with the `Trainer` API to train a model, the easiest way to upload it to the Hub is to set `push_to_hub=True` when you define your `TrainingArguments`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963c2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"bert-finetuned-mrpc\", save_strategy=\"epoch\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c7f2e",
   "metadata": {},
   "source": [
    "When you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time it is saved (here every epoch) in a repository in your namespace. That repository will be named like the output directory you picked (here `bert-finetuned-mrpc`) but you can choose a different name with `hub_model_id = \"a_different_name\"`.\n",
    "\n",
    "To upload you model to an organization you are a member of, just pass it with `hub_model_id = \"my_organization/my_repo_name\"`.\n",
    "\n",
    "Once your training is finished, you should do a final `trainer.push_to_hub()` to upload the last version of your model. It will also generate a model card with all the relevant metadata, reporting the hyperparameters used and the evaluation results! Here is an example of the content you might find in a such a model card:\n",
    "\n",
    "<img style=\"float=center;\" src=\"images/eca78d9c0c7d6a8cc23c08268821469299ff5b2da26e10b233d69ec30bfc698b.png\">\n",
    "\n",
    "At a lower level, accessing the Model Hub can be done directly on models, tokenizers, and configuration objects via their `push_to_hub()` method. This method takes care of both the repository creation and pushing the model and tokenizer files directly to the repository. No manual handling is required, unlike with the API we'll see below.\n",
    "\n",
    "To get an idea of how it works, let's first initialize a model and a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a3daadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"camembert-base\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4bef1",
   "metadata": {},
   "source": [
    "You're free to do whatever you want with these - add tokens to the tokenizer, train the model, fine-tune it. Once you're happy with the resulting model, weights, and tokenizer, you can leverage the `push_to_hub()` method directly available on the `model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed3eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/huggingface_hub/hf_api.py:1001: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/mdroth/dummy-model into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efa875358bd4fd9aec9d9c90aa7872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/mdroth/dummy-model\n",
      "   641b512..ad7161c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mdroth/dummy-model/commit/ad7161c9fc7579b60297435f9d616e903996180b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"dummy-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8629344",
   "metadata": {},
   "source": [
    "This will create the new repository `dummy-model` in your profile, and populate it with your model files. Do the same with the tokenizer, so that all the files are now available in this repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da49b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052495deddf54afe99048a80e2cb8c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file sentencepiece.bpe.model:   4%|4         | 32.0k/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/mdroth/dummy-model\n",
      "   ad7161c..79d77b3  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mdroth/dummy-model/commit/79d77b3fbff050b474c4d704df53c629ac60e29d'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"dummy-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cb374",
   "metadata": {},
   "source": [
    "If you belong to an organization, simply specify the `organization` argument to upload to that organization's namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6fd7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e712c1c",
   "metadata": {},
   "source": [
    "If you wish to use a specific Hugging Face token, you're free to specify it to the `push_to_hub()` method as well:\n",
    "```python\n",
    "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\", use_auth_token=\"<TOKEN>\")\n",
    "```\n",
    "Now head to the Model Hub to find your newly uploaded model: https://huggingface.co/user-or-organization/dummy-model.\n",
    "\n",
    "Click on the \"Files and versions\" tab, and you should see the files visible in the following screenshot:\n",
    "\n",
    "<img style=\"float=center;\" src=\"images/f9159f53992bf432f781e872e33347644e7b2d736fbe2a9c477d0f83250df079.png\">\n",
    "\n",
    "> ‚úèÔ∏è Try it out! <font color=\"darkgreen\">Take the model and tokenizer associated with the `bert-base-cased` checkpoint and upload them to a repo in your namespace using the `push_to_hub()` method. Double-check that the repo appears properly on your page before deleting it.</font>\n",
    "\n",
    "As you've seen, the `push_to_hub()` method accepts several arguments, making it possible to upload to a specific repository or organization namespace, or to use a different API token. We recommend you take a look at the method specification available directly in the [ü§ó Transformers documentation](https://huggingface.co/transformers/model_sharing.html) to get an idea of what is possible.\n",
    "\n",
    "The `push_to_hub()` method is backed by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package, which offers a direct API to the Hugging Face Hub. It's integrated within ü§ó Transformers and several other machine learning libraries, like [`allenlp`](https://github.com/allenai/allennlp). Although we focus on the ü§ó Transformers integration in this chapter, integrating it into your own code or library is simple.\n",
    "\n",
    "Jump to the last section to see how to upload files to your newly created repository!\n",
    "\n",
    "### Using the `huggingface_hub` Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37da15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1677587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
